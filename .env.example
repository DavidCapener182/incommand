# Core environment configuration

# Supabase
NEXT_PUBLIC_SUPABASE_URL=""
SUPABASE_SERVICE_ROLE_KEY=""

# OpenAI
OPENAI_API_KEY=""

# Optional model overrides
OPENAI_SENTIMENT_MODEL="gpt-3.5-turbo"
OPENAI_DEBRIEF_MODEL="gpt-4o-mini"

# Optional: Ollama API key if your server requires auth
OLLAMA_API_KEY=""

# Local LLM Configuration (Ollama)
# These variables are optional and only used when Ollama fallback is enabled
# Set these to configure a local or remote Ollama server and per-feature models

OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL_DEFAULT="llama3.2:latest"
OLLAMA_MODEL_DEBRIEF="llama3.2:latest"
OLLAMA_MODEL_SENTIMENT="llama3.2:latest"
# Optional: Endpoint-specific Ollama models for incident detail generation
# These are only needed if using Ollama as a fallback for incident details.
OLLAMA_MODEL_MEDICAL="llama3.2:latest"
OLLAMA_MODEL_EJECTION="llama3.2:latest"
OLLAMA_MODEL_WELFARE="llama3.2:latest"
OLLAMA_MODEL_INCIDENT="llama3.2:latest"
# Optional: AI Assistant-specific Ollama model
# Used when the AI Chat Assistant falls back to Ollama. If unset, defaults to OLLAMA_MODEL_DEFAULT.
OLLAMA_MODEL_ASSISTANT="llama3.2:latest"

# Optional: Increase this if your local model is slow to respond (milliseconds)
# Default is 10000ms if unset
OLLAMA_TIMEOUT_MS="15000"
